name: Extract components & fetch copyrights + licenses

on:
  workflow_dispatch:
  push:
    branches: [ main, master ]
  pull_request:

permissions:
  contents: read

jobs:
  build-and-scan:
    runs-on: ubuntu-latest
    timeout-minutes: 120

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Validate SBOM files exist
        run: |
          set -euo pipefail
          test -f "sboms/spdx-lite.json" || { echo "Missing sboms/spdx-lite.json" >&2; exit 1; }
          test -f "sboms/cyclonedx.json" || { echo "Missing sboms/cyclonedx.json" >&2; exit 1; }

      - name: Set up Python (3.11 for ScanCode 32.4.x)
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install minimal prerequisites
        run: |
          set -euo pipefail
          sudo apt-get update
          sudo apt-get install -y xz-utils bzip2 zlib1g libxml2-dev libxslt1-dev git-lfs
          git lfs install

      - name: Generate components.json from SBOMs (robust git_url; treat unknown version as null)
        run: |
          set -euo pipefail
          python - << 'PY'
          import json, os, re, urllib.parse

          SPDX_PATH = "sboms/spdx-lite.json"
          CDX_PATH  = "sboms/cyclonedx.json"

          def load_json(path):
              if not os.path.exists(path): return None
              with open(path, 'r', encoding='utf-8') as f: return json.load(f)

          def is_git_host(url: str|None) -> bool:
              if not url: return False
              u = url.lower()
              return ("github.com" in u) or ("gitlab.com" in u) or ("bitbucket.org" in u) or u.startswith(("git://", "git@", "ssh://"))

          def from_purl_to_git(url: str|None) -> str|None:
              if not url: return None
              s = url.strip()
              if s.startswith("pkg:"): s = s[4:]
              s = s.split('#', 1)[0].split('?', 1)[0]
              base = s.rsplit('@', 1)[0]
              m = re.search(r'^(github\\.com|gitlab\\.com|bitbucket\\.org)/([^/]+)/([^/]+)$', base)
              if m:
                  host, owner, repo = m.groups()
                  repo = repo.rstrip('.git')
                  return f"https://{host}/{owner}/{repo}.git"
              if base.startswith(("http://","https://")) and is_git_host(base):
                  u = base.rstrip("/")
                  if not u.endswith(".git"): u += ".git"
                  return u
              return None

          def normalize_git_url(url: str|None) -> str|None:
              if not url: return None
              if url.strip().startswith("pkg:"):
                  converted = from_purl_to_git(url)
                  if converted: return converted
              u = url.strip()
              if u.startswith("http") and is_git_host(u) and not u.endswith(".git"):
                  u = u.rstrip("/") + ".git"
              return u

          def last_segment(name: str|None):
              if not name: return None
              s = name
              for sep in ('/', ':'):
                  if sep in s: s = s.split(sep)[-1]
              return s

          def extract_from_purl(purl):
              if not purl: return (None, None, None)
              s = purl.strip()
              if s.startswith("pkg:"): s = s[4:]
              s = s.split('#', 1)[0].split('?', 1)[0]
              version = None
              base = s
              if '@' in s:
                  base, version = s.rsplit('@', 1)
              component = urllib.parse.unquote(base.split('/')[-1])
              git_url = from_purl_to_git(purl)
              return (component, version, git_url)

          def clean_version(ver: str|None) -> str|None:
              if not ver: return None
              v = ver.strip().lower()
              if v in {"", "unknown", "n/a", "na", "none"}:
                  return None
              return ver.strip()

          def best_git_url_from_spdx(pkg) -> str|None:
              if not pkg: return None
              for ref in pkg.get("externalRefs", []) or []:
                  rtype = (ref.get("referenceType") or ref.get("type") or "").lower()
                  loc = ref.get("referenceLocator") or ref.get("locator") or ""
                  if is_git_host(loc) or rtype in {"vcs", "repository", "scm"}:
                      u = normalize_git_url(loc)
                      if u: return u
              dl = pkg.get("downloadLocation") or ""
              if is_git_host(dl):
                  u = normalize_git_url(dl)
                  if u: return u
              hp = pkg.get("homepage") or ""
              if is_git_host(hp):
                  u = normalize_git_url(hp)
                  if u: return u
              return None

          def best_git_url_from_cdx(comp, purl) -> str|None:
              if comp:
                  for ref in comp.get("externalReferences", []) or []:
                      rtype = (ref.get("type") or "").lower()
                      url = ref.get("url") or ref.get("locator") or ""
                      if is_git_host(url) or rtype in {"vcs", "repository", "scm", "source"}:
                          u = normalize_git_url(url)
                          if u: return u
                  url = comp.get("repository") or ""
                  if is_git_host(url):
                      u = normalize_git_url(url)
                      if u: return u
              name_p, ver_p, git_p = extract_from_purl(purl) if purl else (None, None, None)
              return git_p

          def parse_cyclonedx(path):
              out = []
              data = load_json(path)
              if not data: return out
              for c in data.get("components", []):
                  purl = c.get("purl")
                  name_p, ver_p, git_p = extract_from_purl(purl) if purl else (None, None, None)
                  name = name_p or last_segment(c.get("name"))
                  version = clean_version(c.get("version") or ver_p)
                  git_url = best_git_url_from_cdx(c, purl) or git_p
                  git_url = normalize_git_url(git_url)
                  if name:
                      out.append({"component": name, "version": version, "git_url": git_url})
              return out

          def parse_spdx(path):
              out = []
              data = load_json(path)
              if not data: return out
              pkgs = data.get("packages") or []
              for p in pkgs:
                  name = last_segment(p.get("name") or p.get("packageName"))
                  version = clean_version(p.get("versionInfo"))
                  purl = None
                  for ref in p.get("externalRefs", []) or []:
                      rtype = (ref.get("referenceType") or ref.get("type") or "").lower()
                      loc = ref.get("referenceLocator") or ref.get("locator") or ""
                      if rtype == "purl" and loc:
                          purl = loc
                          break
                  name_p, ver_p, git_p = extract_from_purl(purl) if purl else (None, None, None)
                  if name_p: name = name_p
                  if not version: version = clean_version(ver_p)
                  git_url = best_git_url_from_spdx(p) or git_p
                  git_url = normalize_git_url(git_url)
                  if name:
                      out.append({"component": name, "version": version, "git_url": git_url})
              return out

          # Merge by (component, version), prefer entries with valid git_url
          comps = parse_cyclonedx(CDX_PATH) + parse_spdx(SPDX_PATH)
          seen, result = set(), []
          for c in comps:
              key = (c["component"], c.get("version") or "")
              if key in seen:
                  if c.get("git_url"):
                      for r in result:
                          if (r["component"], r.get("version") or "") == key and not r.get("git_url"):
                              r["git_url"] = c["git_url"]
                  continue
              seen.add(key)
              result.append(c)

          os.makedirs("artifacts", exist_ok=True)
          with open("artifacts/components.json", "w", encoding="utf-8") as f:
              json.dump({"components": result}, f, indent=2, ensure_ascii=False)
          print(f"Wrote artifacts/components.json with {len(result)} components.")
          PY

      - name: Upload components.json
        uses: actions/upload-artifact@v4
        with:
          name: components-json
          path: artifacts/components.json

      - name: Create venv & install ScanCode (PyPI, pinned 32.4.1)
        id: install_scancode
        run: |
          set -euo pipefail
          python -m venv .venv
          . .venv/bin/activate
          python -m pip install --upgrade pip
          pip install "scancode-toolkit==32.4.1"
          .venv/bin/scancode --version
          echo "SCANCODE=.venv/bin/scancode" >> "$GITHUB_ENV"

      - name: Clone repos and run ScanCode (copyright + license) with per-component summary
        env:
          SCANCODE: ${{ env.SCANCODE }}
        run: |
          set -euo pipefail
          python - << 'PY'
          import json, os, subprocess, shlex, re

          SCANCODE = os.environ["SCANCODE"]
          SUMMARY_PATH = os.environ.get("GITHUB_STEP_SUMMARY")

          with open("artifacts/components.json", "r", encoding="utf-8") as f:
              comps = json.load(f).get("components", [])

          os.makedirs("repos", exist_ok=True)
          os.makedirs("scancode_results", exist_ok=True)

          def sanitize(s): return re.sub(r'[^A-Za-z0-9_.-]+', '-', (s or "")).strip('-') or "unknown"
          def run(cmd):
              print("+", cmd)
              return subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)

          md = []
          md.append("## Component-wise Scan Summary")
          md.append("")
          md.append("| Component | Version | Repo URL | Checkout | Status | Files | Errors | #Copyrights | Licenses |")
          md.append("|---|---|---|---|---|---:|---:|---:|---|")

          results = []
          for item in comps:
              name = item.get("component")
              version = item.get("version")  # None => default branch
              git_url = item.get("git_url")

              if not name:
                  continue

              checkout_desc = "default branch" if not version else f"tag/branch `{version}`"
              status = "skipped"
              files_count = 0
              errors_count = 0
              copy_count = 0
              license_keys_preview = "-"

              if not git_url:
                  results.append({"component": name, "version": version, "git_url": git_url,
                                  "copyrights": {"unique_statements": [], "per_file": []},
                                  "licenses": {"unique_keys": [], "per_file": []}})
                  md.append(f"| `{name}` | {version or ''} | *(none)* | {checkout_desc} | skipped | 0 | 0 | 0 | - |")
                  continue

              dest = os.path.join("repos", sanitize(name) + (f"-{sanitize(version)}" if version else ""))
              if not os.path.exists(dest):
                  if version:
                      p = run(f"git clone --depth 1 --branch {shlex.quote(version)} {shlex.quote(git_url)} {shlex.quote(dest)}")
                      if p.returncode != 0:
                          print(p.stdout)
                          status = "clone failed"
                          results.append({"component": name, "version": version, "git_url": git_url,
                                          "copyrights": {"unique_statements": [], "per_file": []},
                                          "licenses": {"unique_keys": [], "per_file": []}})
                          md.append(f"| `{name}` | {version or ''} | {git_url} | {checkout_desc} | {status} | 0 | 0 | 0 | - |")
                          continue
                  else:
                      p = run(f"git clone --depth 1 {shlex.quote(git_url)} {shlex.quote(dest)}")
                      if p.returncode != 0:
                          print(p.stdout)
                          status = "clone failed"
                          results.append({"component": name, "version": version, "git_url": git_url,
                                          "copyrights": {"unique_statements": [], "per_file": []},
                                          "licenses": {"unique_keys": [], "per_file": []}})
                          md.append(f"| `{name}` | {version or ''} | {git_url} | {checkout_desc} | {status} | 0 | 0 | 0 | - |")
                          continue

              out_json = os.path.join("scancode_results", sanitize(name) + ".json")
              cmd = f"{shlex.quote(SCANCODE)} --copyright --license --strip-root --json-pp {shlex.quote(out_json)} {shlex.quote(dest)}"
              p = run(cmd)
              if p.returncode != 0:
                  print(p.stdout)

              unique_copyrights = set()
              per_file_copyrights = []
              unique_license_keys = set()
              per_file_licenses = []

              try:
                  with open(out_json, "r", encoding="utf-8") as f:
                      data = json.load(f)

                  headers = data.get("headers", [])
                  if headers:
                      h0 = headers[0]
                      files_count = h0.get("files_count") or h0.get("resource_count") or 0
                      errors_count = len(h0.get("errors", []) or [])
                  if not files_count:
                      files_count = len(data.get("files", []) or [])

                  for fe in data.get("files", []) or []:
                      path = fe.get("path") or fe.get("location") or ""

                      # Copyrights (statements + holders)
                      file_statements = []
                      holders = []
                      for c in fe.get("copyrights", []) or []:
                          for stmt in c.get("statements", []) or []:
                              s = (stmt or "").strip()
                              if s:
                                  file_statements.append(s)
                                  unique_copyrights.add(s)
                          for h in c.get("holders", []) or []:
                              h = (h or "").strip()
                              if h:
                                  holders.append(h)
                      if file_statements or holders:
                          per_file_copyrights.append({
                              "path": path,
                              "statements": file_statements,
                              "holders": holders
                          })

                      # Licenses: legacy "licenses", expressions, and v32 detections
                      for lic in fe.get("licenses", []) or []:
                          key = lic.get("key")
                          if key:
                              unique_license_keys.add(key)

                      expr = fe.get("detected_license_expression") or fe.get("license_expression") or ""
                      if expr:
                          for token in re.split(r'\s+(AND|OR|WITH)\s+', expr):
                              token = token.strip()
                              if token and token not in {"AND","OR","WITH"}:
                                  unique_license_keys.add(token)

                      for det in fe.get("license_detections", []) or []:
                          k = det.get("license_expression") or det.get("license_expression_spdx") or ""
                          if k:
                              for token in re.split(r'\s+(AND|OR|WITH)\s+', k):
                                  token = token.strip()
                                  if token and token not in {"AND","OR","WITH"}:
                                      unique_license_keys.add(token)
                          per_file_licenses.append({
                              "path": path,
                              "detected_expression": det.get("license_expression") or det.get("license_expression_spdx"),
                              "matches": det.get("matches", [])
                          })

                  # codebase-level unique license detections (if present)
                  for det in data.get("license_detections", []) or []:
                      k = det.get("license_expression") or det.get("license_expression_spdx") or ""
                      if k:
                          for token in re.split(r'\s+(AND|OR|WITH)\s+', k):
                              token = token.strip()
                              if token and token not in {"AND","OR","WITH"}:
                                  unique_license_keys.add(token)

              except Exception as e:
                  print(f"Parse failed for {name}: {e}")

              copy_count = len(unique_copyrights)
              status = "scanned"
              license_keys_preview = "-"
              if unique_license_keys:
                  preview = sorted(unique_license_keys)[:3]
                  license_keys_preview = ", ".join(preview) + ("â€¦" if len(unique_license_keys) > 3 else "")

              results.append({
                  "component": name,
                  "version": version,
                  "git_url": git_url,
                  "copyrights": {
                      "unique_statements": sorted(unique_copyrights),
                      "per_file": per_file_copyrights
                  },
                  "licenses": {
                      "unique_keys": sorted(unique_license_keys),
                      "per_file": per_file_licenses
                  }
              })

              md.append(f"| `{name}` | {version or ''} | {git_url} | {checkout_desc} | {status} | {files_count} | {errors_count} | {copy_count} | {license_keys_preview or '-'} |")

          os.makedirs("artifacts", exist_ok=True)
          with open("artifacts/component_copyrights.json", "w", encoding="utf-8") as f:
              json.dump({"components": results}, f, indent=2, ensure_ascii=False)

          if SUMMARY_PATH:
              with open(SUMMARY_PATH, "a", encoding="utf-8") as sf:
                  sf.write("\n".join(md) + "\n")

          print(f"Wrote artifacts/component_copyrights.json for {len(results)} components.")
          PY

      - name: Upload raw ScanCode outputs (per-repo)
        uses: actions/upload-artifact@v4
        with:
          name: scancode-raw
          path: scancode_results/*.json
          if-no-files-found: ignore
          retention-days: 7

      - name: Upload final aggregated copyrights + licenses (JSON)
        uses: actions/upload-artifact@v4
        with:
          name: component-copyrights-licenses
          path: artifacts/component_copyrights.json
          if-no-files-found: error
          retention-days: 30

      - name: Build per-component attribution file (YAML)
        run: |
          set -euo pipefail
          python - << 'PY'
          import json, os, re, sys

          def sanitize(s):
              import re
              return re.sub(r'[^A-Za-z0-9_.-]+', '-', (s or "")).strip('-') or "unknown"

          def esc_yaml(s):
              if s is None: 
                  return '""'
              if re.search(r'[:#\\-\\[\\]\\{\\},&*?]|^\\s|\\s$', s):
                  return '"' + s.replace('"', '\\"') + '"'
              return s

          def write_yaml(items, path):
              lines = []
              lines.append("components:")
              for it in items:
                  lines.append("  - component: " + esc_yaml(it["component"]))
                  if it.get("version") is not None:
                      lines.append("    version: " + esc_yaml(it["version"]))
                  lines.append("    url: " + esc_yaml(it.get("url") or ""))
                  lines.append("    license: " + esc_yaml(it.get("license") or ""))
                  cps = it.get("copyrights") or []
                  if cps:
                      lines.append("    copyrights:")
                      for c in cps:
                          lines.append("      - " + esc_yaml(c))
                  else:
                      lines.append("    copyrights: []")
              with open(path, "w", encoding="utf-8") as f:
                  f.write("\\n".join(lines) + "\\n")

          try:
              with open("artifacts/components.json", "r", encoding="utf-8") as f:
                  components = json.load(f).get("components", [])
          except Exception as e:
              print(f"FATAL: cannot read artifacts/components.json: {e}")
              sys.exit(1)

          agg = {}
          try:
              with open("artifacts/component_copyrights.json", "r", encoding="utf-8") as f:
                  agg_data = json.load(f)
              for c in (agg_data.get("components") or []):
                  agg[c.get("component")] = c
          except Exception as e:
              print(f"WARNING: cannot read aggregated JSON, will use raw only: {e}")

          def load_raw(component_name):
              p = os.path.join("scancode_results", sanitize(component_name) + ".json")
              if os.path.exists(p):
                  try:
                      with open(p, "r", encoding="utf-8") as f:
                          return json.load(f)
                  except Exception as e:
                      print(f"WARNING: failed to parse raw JSON for {component_name}: {e}")
              else:
                  print(f"INFO: raw JSON not found for {component_name}: {p}")
              return None

          def top_spdx_license_from_data(data):
              counts = {}
              def bump(key):
                  if not key: return
                  k = key.strip()
                  if k:
                      counts[k] = counts.get(k, 0) + 1
              if not data:
                  return None
              for fe in data.get("files", []) or []:
                  bump(fe.get("detected_license_expression_spdx"))
                  for det in fe.get("license_detections", []) or []:
                      bump(det.get("license_expression_spdx"))
                  for lic in fe.get("licenses", []) or []:
                      bump(lic.get("spdx_license_key"))
              for det in data.get("license_detections", []) or []:
                  bump(det.get("license_expression_spdx"))
              if not counts: 
                  return None
              return sorted(counts.items(), key=lambda kv: (-kv[1], kv[0]))[0][0]

          def top_spdx_license_from_agg(agg_entry):
              counts = {}
              if not agg_entry:
                  return None
              for k in agg_entry.get("licenses", {}).get("unique_keys", []) or []:
                  if k:
                      counts[k] = counts.get(k, 0) + 1
              if not counts:
                  return None
              return sorted(counts.items(), key=lambda kv: (-kv[1], kv[0]))[0][0]

          def collect_copyrights_from_data(data):
              out = set()
              if not data:
                  return []
              for fe in data.get("files", []) or []:
                  for c in fe.get("copyrights", []) or []:
                      val = (c.get("copyright") or "").strip()
                      if val:
                          out.add(val)
              return sorted(out)

          def collect_copyrights_from_agg(agg_entry):
              if not agg_entry:
                  return []
              return list(agg_entry.get("copyrights", {}).get("unique_statements", []))

          entries = []
          for comp in components:
              name = comp.get("component")
              version = comp.get("version")
              url = comp.get("git_url")

              raw = load_raw(name)
              agg_entry = agg.get(name)

              primary_spdx = top_spdx_license_from_data(raw) or top_spdx_license_from_agg(agg_entry)
              copyrights = collect_copyrights_from_data(raw) or collect_copyrights_from_agg(agg_entry)

              entries.append({
                  "component": name,
                  "version": version,
                  "url": url,
                  "license": primary_spdx,
                  "copyrights": copyrights,
              })

          os.makedirs("artifacts", exist_ok=True)
          out_yaml = "artifacts/component_attribution.yaml"
          write_yaml(entries, out_yaml)
          print(f"Wrote {out_yaml} with {len(entries)} components.")
          PY

      - name: Upload per-component attribution (YAML)
        uses: actions/upload-artifact@v4
        with:
          name: component-attribution
          path: artifacts/component_attribution.yaml
          if-no-files-found: error
          retention-days: 30

      - name: Build per-component attribution file (TXT, line-by-line)
        run: |
          set -euo pipefail
          python - << 'PY'
          import json, os, sys, re

          # Load aggregated JSON produced earlier
          try:
              with open("artifacts/component_copyrights.json", "r", encoding="utf-8") as f:
                  agg = json.load(f)
          except Exception as e:
              print(f"FATAL: cannot read aggregated JSON: {e}")
              sys.exit(1)

          comps = agg.get("components", [])
          lines = []
          for c in comps:
              name = c.get("component") or ""
              version = c.get("version")
              url = c.get("git_url") or ""
              # primary license = first of unique_keys if present
              keys = c.get("licenses", {}).get("unique_keys", []) or []
              license_str = keys[0] if keys else ""

              lines.append(f"Component: {name}")
              if version:
                  lines.append(f"Version: {version}")
              lines.append(f"URL: {url}")
              lines.append(f"License: {license_str}")
              lines.append("Copyrights:")

              cps = c.get("copyrights", {}).get("unique_statements", []) or []
              if cps:
                  for stmt in cps:
                      # normalize angle braces to readable form
                      stmt = re.sub(r'<', '&lt;', stmt)
                      stmt = re.sub(r'>', '&gt;', stmt)
                      lines.append(f"- {stmt}")
              else:
                  lines.append("- (none)")

              lines.append("")  # blank line between components

          os.makedirs("artifacts", exist_ok=True)
          out_txt = "artifacts/component_attribution.txt"
          with open(out_txt, "w", encoding="utf-8") as f:
              f.write("\n".join(lines).rstrip() + "\n")
          print(f"Wrote {out_txt} with {len(comps)} components.")
          PY

      - name: Upload per-component attribution (TXT)
        uses: actions/upload-artifact@v4
        with:
          name: component-attribution-text
          path: artifacts/component_attribution.txt
          if-no-files-found: error
          retention-days: 30
