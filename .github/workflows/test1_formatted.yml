name: test1_formatted Generate NOTICE

on:
  workflow_dispatch:
    inputs:
      sbom_paths:
        description: "Comma-separated SBOM JSON paths (SPDX or CycloneDX)"
        required: true
        default: "sboms/spdx-lite.json,sboms/cyclonedx.json"
      notice_title:
        description: "Title for NOTICE.md"
        required: false
        default: "Open Source Notices"
  push:
    branches: [ main, master ]
    paths:
      - "sboms/**/*.json"
      - "sbom*.json"

# NOTE: No 'permissions: contents: write' needed since we don't push to repo.

jobs:
  # ---------------------------------------------------------
  # Job 1: Prepare — parse SBOMs, chunk into pages of 4
  # ---------------------------------------------------------
  prepare:
    runs-on: ubuntu-latest
    outputs:
      pages_json: ${{ steps.mk.outputs.pages_json }}   # matrix for page jobs
      page_size:  ${{ steps.mk.outputs.page_size }}
      total:      ${{ steps.mk.outputs.total }}
      title:      ${{ steps.mk.outputs.title }}
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install parsers
        run: |
          python -m pip install --upgrade pip
          pip install requests packaging cyclonedx-python-lib spdx-tools

      - name: Build pages from SBOMs (safe names; 4 per page)
        id: mk
        run: |
          python - <<'PY'
          import json, os, re
          from pathlib import Path
          NOASSERT={"NOASSERTION","NONE","",None}

          def norm(s):
              if s is None: return None
              s=str(s).strip()
              return None if (not s or s.upper() in NOASSERT) else " ".join(s.split())

          def detect(doc):
              if isinstance(doc, dict):
                  if doc.get("bomFormat")=="CycloneDX" or "components" in doc: return "cdx"
                  if doc.get("spdxVersion") or doc.get("SPDXID") or "packages" in doc or "files" in doc: return "spdx"
              return "unknown"

          def parse_spdx(doc):
              res=[]
              for p in doc.get("packages") or []:
                  name=norm(p.get("name"))
                  if not name: continue
                  version=norm(p.get("versionInfo"))
                  lic=norm(p.get("licenseConcluded")) or norm(p.get("licenseDeclared"))
                  if not lic:
                      infos=p.get("licenseInfoFromFiles") or []
                      toks=sorted(set([norm(x) for x in infos if norm(x)]))
                      lic=" AND ".join(toks) if toks else None
                  homepage=norm(p.get("homepage"))
                  dl=norm(p.get("downloadLocation"))
                  url=dl or homepage
                  purl=None
                  for ref in p.get("externalRefs") or []:
                      rtype=(ref.get("referenceType") or "").lower()
                      loc=norm(ref.get("referenceLocator"))
                      if "purl" in rtype and loc:
                          purl=loc; break
                  res.append({"name":name,"version":version,"license":lic,"url":url,"purl":purl})
              return res

          def parse_cdx(doc):
              res=[]
              for c in doc.get("components") or []:
                  name=norm(c.get("name"))
                  if not name: continue
                  version=norm(c.get("version"))
                  purl=norm(c.get("purl"))
                  lic=None
                  if c.get("licenses"):
                      exprs=[norm(x.get("expression")) for x in c["licenses"] if isinstance(x,dict) and x.get("expression")]
                      if exprs and exprs[0]: lic=exprs[0]
                      else:
                          ids=[]
                          for entry in c["licenses"]:
                              licd=entry.get("license") if isinstance(entry,dict) else None
                              if isinstance(licd,dict):
                                  lid=norm(licd.get("id")); lname=norm(licd.get("name"))
                                  if lid: ids.append(lid)
                                  elif lname: ids.append(lname)
                          ids=sorted(set(ids))
                          lic=" AND ".join(ids) if ids else None
                  url=None
                  for ref in c.get("externalReferences") or []:
                      rtype=(ref.get("type") or "").lower(); u=norm(ref.get("url"))
                      if rtype in {"website","vcs","distribution","documentation","release-notes"} and u:
                          url=u; break
                  res.append({"name":name,"version":version,"license":lic,"url":url,"purl":purl})
              return res

          def slug(s: str, maxlen: int = 120) -> str:
              if not s: return "component"
              s = re.sub(r'["<>|*?:\\/\r\n]', '-', s)
              s = re.sub(r'\s+', '-', s)
              s = re.sub(r'-{2,}', '-', s).strip('-')
              s = re.sub(r'[^A-Za-z0-9._-]+', '-', s)
              s = s[:maxlen].strip('-')
              return s or "component"

          sboms = os.environ.get("SBOMS","sboms/spdx-lite.json,sboms/cyclonedx.json").split(",")
          comps=[]
          import json as _json
          for path in [s.strip() for s in sboms if s.strip()]:
              with open(path,"r",encoding="utf-8") as f:
                  doc=_json.load(f)
              kind=detect(doc)
              if kind=="spdx": comps+=parse_spdx(doc)
              elif kind=="cdx": comps+=parse_cdx(doc)

          # de-dupe by purl else name@version
          merged={}
          for c in comps:
              key=("purl",c.get("purl")) if c.get("purl") else ("nv",f"{(c.get('name') or '').lower()}@{(c.get('version') or '').lower()}")
              if key not in merged: merged[key]=c
              else:
                  for fld in ("license","url","version"):
                      if not merged[key].get(fld) and c.get(fld): merged[key][fld]=c[fld]

          items=list(merged.values())
          for i,c in enumerate(items):
              base = c.get("purl") or c.get("name") or f"component-{i}"
              c["safe_name"] = slug(base)
              c["idx"] = i

          total=len(items)
          PAGE_SIZE=int(os.environ.get("PAGE_SIZE","4"))  # 4 per page
          pages=[]
          for pidx in range(0, total, PAGE_SIZE):
              page_items = items[pidx:pidx+PAGE_SIZE]
              pages.append({
                  "page_index": len(pages),
                  "count": len(page_items),
                  "components": page_items
              })

          # matrix for page jobs: include page_index + count
          matrix={"include":[{"page_index": i, "count": pages[i]["count"]} for i in range(len(pages))]}

          # outputs
          out = os.environ["GITHUB_OUTPUT"]
          with open(out,"a",encoding="utf-8") as gh:
              gh.write(f"pages_json={json.dumps(matrix, ensure_ascii=False)}\n")
              gh.write(f"page_size={PAGE_SIZE}\n")
              gh.write(f"total={total}\n")
              gh.write(f"title={os.environ.get('TITLE','Open Source Notices')}\n")

          # dump full pages payload to workspace for scan-page jobs
          Path("pages.json").write_text(json.dumps({"pages": pages}, ensure_ascii=False), encoding="utf-8")
          PY
        env:
          SBOMS: "${{ github.event.inputs.sbom_paths || 'sboms/spdx-lite.json,sboms/cyclonedx.json' }}"
          TITLE: "${{ github.event.inputs.notice_title || 'Open Source Notices' }}"
          PAGE_SIZE: "4"

      - name: Upload pages.json (for scan-page jobs)
        uses: actions/upload-artifact@v4
        with:
          name: pages
          path: pages.json
          retention-days: 7

      - name: Prep summary
        run: |
          echo "### Prepared components" >> "$GITHUB_STEP_SUMMARY"
          echo "- Total components: **${{ steps.mk.outputs.total }}**" >> "$GITHUB_STEP_SUMMARY"
          echo "- Page size: **${{ steps.mk.outputs.page_size }}**" >> "$GITHUB_STEP_SUMMARY"
          echo "- Pages: **$(( ( ${{ steps.mk.outputs.total }} + ${{ steps.mk.outputs.page_size }} - 1 ) / ${{ steps.mk.outputs.page_size }} ))**" >> "$GITHUB_STEP_SUMMARY"

  # ---------------------------------------------------------
  # Job 2: Scan-page — matrix over pages (each scans 4 comps sequentially)
  # ---------------------------------------------------------
  scan-page:
    needs: prepare
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.prepare.outputs.pages_json) }}
      max-parallel: 64  # run many pages at once; tune per runner capacity
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - uses: actions/download-artifact@v4
        with:
          name: pages
          path: .

      - name: Install ScanCode Toolkit
        run: |
          python -m pip install --upgrade pip
          pip install scancode-toolkit requests packaging

      - name: Scan all 4 components in this page (sequential; progress lines)
        id: scan
        shell: bash
        run: |
          python - <<'PY'
          import os, json, re, requests, tarfile, zipfile, subprocess
          from pathlib import Path

          REQ_TIMEOUT=45

          # Load the full pages payload
          pages = json.loads(Path("pages.json").read_text(encoding="utf-8"))["pages"]
          page_index = int(os.environ["PAGE_INDEX"])
          comps = pages[page_index]["components"]
          total = len(comps)

          # Summary header for this page
          print(f"echo '### Page {page_index} summary' >> $GITHUB_STEP_SUMMARY")
          print(f"echo '- Components in page: **{total}**' >> $GITHUB_STEP_SUMMARY")

          def try_codeload_zip(owner, repo, ref):
              u=f"https://codeload.github.com/{owner}/{repo}/zip/refs/heads/{ref}"
              r=requests.get(u,timeout=REQ_TIMEOUT)
              return r.content if r.status_code==200 else None

          def try_tag_zip(owner, repo, tag):
              u=f"https://codeload.github.com/{owner}/{repo}/zip/refs/tags/{tag}"
              r=requests.get(u,timeout=REQ_TIMEOUT)
              return r.content if r.status_code==200 else None

          out_ok = 0
          success_lines = []

          for i, c in enumerate(comps, 1):
              name=c.get("name") or ""
              version=c.get("version") or ""
              url=c.get("url") or ""
              purl=c.get("purl") or ""
              safe=c.get("safe_name") or f"component-{i}"
              work=Path(".scancode_work")/safe
              work.mkdir(parents=True, exist_ok=True)
              fn=None

              def get(url): return requests.get(url,timeout=REQ_TIMEOUT)

              # --- Download/archive selection ---
              try:
                  if purl.startswith("pkg:npm/"):
                      pkg=purl.split("/",2)[-1].split("@")[0]
                      ver=version or (purl.split("@")[-1] if "@" in purl else None)
                      meta=get(f"https://registry.npmjs.org/{pkg}/{ver or 'latest'}").json()
                      buf=get(meta["dist"]["tarball"]).content
                      fn=work/f"{pkg}-{ver or 'latest'}.tgz"; fn.write_bytes(buf)

                  elif purl.startswith("pkg:pypi/"):
                      pkg=purl.split("/",2)[-1].split("@")[0]
                      ver=version or (purl.split("@")[-1] if "@" in purl else None)
                      u=f"https://pypi.org/pypi/{pkg}/{ver}/json" if ver else f"https://pypi.org/pypi/{pkg}/json"
                      meta=get(u).json()
                      urls=meta.get("urls",[])
                      sdist=next((u for u in urls if u.get("packagetype")=="sdist"), None) or (urls[0] if urls else None)
                      if sdist:
                          buf=get(sdist["url"]).content
                          fn=work/sdist["filename"]; fn.write_bytes(buf)

                  elif purl.startswith("pkg:maven/") and version:
                      rest=purl[len("pkg:maven/"):]
                      coords=rest.split("@")[0].split("/")
                      if len(coords)>=2:
                          group,artifact=coords[0],coords[1]
                          base=f"https://repo1.maven.org/maven2/{group.replace('.','/')}/{artifact}/{version}"
                          for suffix in (f"{artifact}-{version}-sources.jar", f"{artifact}-{version}.jar"):
                              u=f"{base}/{suffix}"
                              r=get(u)
                              if r.status_code==200:
                                  fn=work/suffix; fn.write_bytes(r.content); break

                  elif purl.startswith("pkg:nuget/") and version:
                      pkg=purl.split("/",2)[-1].split("@")[0]
                      lower=pkg.lower()
                      u=f"https://api.nuget.org/v3-flatcontainer/{lower}/{version}/{lower}.{version}.nupkg"
                      r=get(u)
                      if r.status_code==200:
                          fn=work/f"{lower}.{version}.nupkg"; fn.write_bytes(r.content)

                  elif purl.startswith("pkg:gem/") and version:
                      pkg=purl.split("/",2)[-1].split("@")[0]
                      u=f"https://rubygems.org/downloads/{pkg}-{version}.gem"
                      r=get(u)
                      if r.status_code==200:
                          fn=work/f"{pkg}-{version}.gem"; fn.write_bytes(r.content)

                  elif purl.startswith("pkg:golang/") and version:
                      mod=purl[len("pkg:golang/"):].split("@")[0]
                      u=f"https://proxy.golang.org/{mod}/@v/{version}.zip"
                      r=get(u)
                      if r.status_code==200:
                          fn=work/f"{mod.replace('/','_')}@{version}.zip"; fn.write_bytes(r.content)

                  # GitHub fallback (default branch, common branches, tag)
                  if fn is None and url:
                      m=re.match(r"https?://github\.com/([^/]+)/([^/?#]+)", url)
                      if m:
                          owner, repo = m.group(1), m.group(2)
                          if version and re.match(r"^[A-Za-z0-9._\-+/]+$", version):
                              buf = try_tag_zip(owner, repo, version)
                              if buf:
                                  zf=work/f"{repo}-{version}.zip"; zf.write_bytes(buf); fn=zf
                          if fn is None:
                              try:
                                  api=f"https://api.github.com/repos/{owner}/{repo}"
                                  r=requests.get(api, timeout=REQ_TIMEOUT)
                                  if r.status_code==200:
                                      ref=r.json().get("default_branch")
                                      if ref:
                                          buf = try_codeload_zip(owner, repo, ref)
                                          if buf:
                                              zf=work/f"{repo}-{ref}.zip"; zf.write_bytes(buf); fn=zf
                              except Exception:
                                  pass
                          if fn is None:
                              for ref in ("HEAD","main","master","develop","stable","staging"):
                                  buf = try_codeload_zip(owner, repo, ref)
                                  if buf:
                                      zf=work/f"{repo}-{ref}.zip"; zf.write_bytes(buf); fn=zf; break
              except Exception as e:
                  print(f"echo '- {i}/4 **{name} {version}**: download error' >> $GITHUB_STEP_SUMMARY")

              if fn is None:
                  print(f"echo '- {i}/4 **{name} {version}**: download failed' >> $GITHUB_STEP_SUMMARY")
                  continue

              # Extract
              src = work/"src"
              ok=False
              try:
                  src.mkdir(parents=True, exist_ok=True)
                  if fn.suffix in (".tgz",".gz",".tar"):
                      with tarfile.open(fn,"r:*") as tf: tf.extractall(src); ok=True
                  else:
                      with zipfile.ZipFile(fn) as zf: zf.extractall(src); ok=True
              except Exception as e:
                  print(f"echo '- {i}/4 **{name} {version}**: extract failed' >> $GITHUB_STEP_SUMMARY")
                  continue

              # ScanCode
              out_json = work/"scan.json"
              try:
                  subprocess.run(
                      ["scancode","-cl","--license-text","--json-pp",str(out_json),str(src)],
                      check=False
                  )
              except Exception as e:
                  print(f"echo '- {i}/4 **{name} {version}**: scan failed' >> $GITHUB_STEP_SUMMARY")
                  continue

              # meta.json
              meta = {
                  "name": name, "version": version, "url": url,
                  "license": c.get("license") or "", "purl": purl,
                  "safe_name": safe, "idx": c.get("idx")
              }
              (work/"meta.json").write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")

              if out_json.exists() and (work/"meta.json").exists():
                  out_ok += 1
                  success_lines.append(f"{work}/scan.json {work}/meta.json")
                  print(f"echo '- {i}/4 **{name} {version}**: OK' >> $GITHUB_STEP_SUMMARY")
              else:
                  print(f"echo '- {i}/4 **{name} {version}**: missing outputs' >> $GITHUB_STEP_SUMMARY")

          # Write success list for packager
          Path("artifacts.lst").write_text("\n".join(success_lines), encoding="utf-8")

          # Export counters
          with open(os.environ["GITHUB_OUTPUT"],"a") as gh:
              gh.write(f"produced={out_ok}\n")

          # Final per-page recap
          print(f"echo '- Outputs (scan.json+meta.json): **{out_ok}/{total}**' >> $GITHUB_STEP_SUMMARY")
          PY
        env:
          PAGE_INDEX: ${{ matrix.page_index }}

      - name: Package per-component outputs in this page
        shell: bash
        run: |
          set -e
          mkdir -p page_out
          if [ ! -s artifacts.lst ]; then
            echo "No successful outputs in this page; skipping packaging."
            exit 0
          fi
          while read -r line; do
            sjson="$(echo "$line" | awk '{print $1}')"
            d="$(dirname "$sjson")"
            base="$(basename "$d")"
            tar -czf "page_out/scan-${{ matrix.page_index }}-${base}.tgz" -C "$d" scan.json meta.json
            echo "Packaged: scan-${{ matrix.page_index }}-${base}.tgz"
          done < artifacts.lst

      - name: Upload compressed page artifacts
        uses: actions/upload-artifact@v4
        with:
          name: page-${{ matrix.page_index }}
          path: page_out/*.tgz
          if-no-files-found: ignore
          retention-days: 14

      - name: Diagnostics (page out & work tree)
        shell: bash
        run: |
          echo "== page_out listing =="; ls -la page_out || true
          echo "== .scancode_work tree (depth 2) =="; find .scancode_work -maxdepth 2 -type d -print || true

      - name: Page summary
        run: |
          echo "### Page ${{ matrix.page_index }} scanned" >> "$GITHUB_STEP_SUMMARY"
          echo "- Components with outputs: **${{ steps.scan.outputs.produced }}**" >> "$GITHUB_STEP_SUMMARY"

  # ---------------------------------------------------------
  # Job 3: Merge — download artifacts, build NOTICE (artifact only)
  # ---------------------------------------------------------
  merge:
    needs: [prepare, scan-page]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Download all page artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: "page-*"
          merge-multiple: true
          path: page_artifacts

      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install helper libs
        run: |
          python -m pip install --upgrade pip
          pip install requests packaging

      - name: Unpack and build NOTICE.md
        id: build
        run: |
          mkdir -p scans
          for f in page_artifacts/*.tgz; do
            [ -f "$f" ] && tar -xzf "$f" -C scans || true
          done

          python - <<'PY'
          import json, os
          from pathlib import Path

          title = "${{ needs.prepare.outputs.title }}"
          scans_dir = Path("scans")
          rows = []
          license_texts = {}

          # Each unpacked directory contains scan.json + meta.json
          for p in scans_dir.glob("**/scan.json"):
              meta = p.parent/"meta.json"
              data = json.loads(p.read_text(encoding="utf-8"))
              meta_d = {}
              if meta.exists():
                  meta_d = json.loads(meta.read_text(encoding="utf-8"))

              name    = meta_d.get("name") or p.parent.name
              version = meta_d.get("version") or ""
              url     = meta_d.get("url") or ""
              lic     = meta_d.get("license") or ""

              cps=[]
              ltexts={}
              for f in data.get("files",[]):
                  for cpr in f.get("copyrights",[]):
                      v=cpr.get("value")
                      if v: cps.append(v.strip())
                  for det in f.get("license_detections",[]):
                      key = det.get("license_expression_spdx") or det.get("license_expression") or det.get("license_key")
                      for m in det.get("matches",[]) or []:
                          t=(m.get("matched_text") or "").strip()
                          if t and key and key not in ltexts:
                              ltexts[key]=t

              seen=set(); cps_u=[]
              for ln in cps:
                  if ln not in seen:
                      seen.add(ln); cps_u.append(ln)

              rows.append({
                "name": name, "version": version, "url": url,
                "license": lic, "copyright": "\n".join(cps_u)
              })
              for k,v in ltexts.items():
                  license_texts.setdefault(k, v)

          out=[]
          out.append(f"# {title}\n")
          for r in rows:
              out.append(f"### {r['name']}" + (f" {r['version']}" if r.get('version') else ""))
              if r.get("url"): out.append(f"- **URL:** {r['url']}")
              if r.get("license"): out.append(f"- **License:** {r['license']}")
              if r.get("copyright"):
                  out.append(f"- **Copyright:** {r['copyright']}")
              out.append("")

          if license_texts:
              out.append("\n## License Texts\n")
              for lid,text in sorted(license_texts.items()):
                  out.append(f"### {lid}\n```text\n{text.strip()}\n```\n")

          Path("NOTICE.md").write_text(("\n".join(out)).rstrip()+"\n", encoding="utf-8")
          with open(os.environ["GITHUB_OUTPUT"],"a") as gh:
              gh.write(f"count={len(rows)}\n")
          PY

      - uses: actions/upload-artifact@v4
        with:
          name: notice-md
          path: NOTICE.md
          retention-days: 30

      - name: Run summary
        run: |
          {
            echo "## NOTICE build summary";
            echo "- Components scanned: **${{ steps.build.outputs.count }}**";
            echo "- Page size: **${{ needs.prepare.outputs.page_size }}** (4 per page)";
            echo "- Pages run in parallel (max): **64**";
            echo "- Download artifact: **notice-md** (contains NOTICE.md)";
          } >> "$GITHUB_STEP_SUMMARY"
